{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install simpletransformers","metadata":{"id":"aviFd6WwmvMd","outputId":"4f8dd270-689d-45cc-c3f2-784138f0f0f5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.compat.v1 as tf1\nimport numpy as np\nfrom simpletransformers.ner import NERModel, NERArgs\n\ntf1.disable_v2_behavior()\n\nclass BiLSTMModel():\n\n    def __init__(self, vocab_size, n_tags, embedding_dim, n_hidden,\n                 padding_idx):\n        self.declare_placeholders()\n        self.build_layers(vocab_size, embedding_dim, n_hidden, n_tags)\n        self.compute_predictions()\n        self.compute_loss(n_tags, padding_idx)\n        self.optimize()\n\n    def declare_placeholders(self):\n        self.input_batch = tf1.placeholder(dtype=tf.int32, shape=[None, None],\n                                           name='input_batch')\n        self.true_tags = tf1.placeholder(dtype=tf.int32, shape=[None, None],\n                                         name='true_tags')\n        self.lengths = tf1.placeholder(dtype=tf.int32, shape=[None],\n                                       name='lengths')\n        self.dropout = tf1.placeholder_with_default(tf.cast(1.0, tf.float32),\n                                                    shape=[])\n        self.learn_rate = tf1.placeholder(dtype=tf.float32, shape=[])\n\n    def build_layers(self, vocab_size, embedding_dim, n_hidden, n_tags):\n        embedding_matrix = np.random.randn(\n            vocab_size, embedding_dim) / np.sqrt(embedding_dim)\n        embedding_matrix_variable = tf.Variable(initial_value=embedding_matrix,\n                                                dtype=tf.float32)\n\n        forward_cell = tf1.nn.rnn_cell.DropoutWrapper(\n            tf1.nn.rnn_cell.LSTMCell(n_hidden), self.dropout, self.dropout)\n        backward_cell = tf1.nn.rnn_cell.DropoutWrapper(\n            tf1.nn.rnn_cell.LSTMCell(n_hidden), self.dropout, self.dropout)\n\n        embeddings = tf.nn.embedding_lookup(embedding_matrix_variable,\n                                            self.input_batch)\n\n        (rnn_output_fw, rnn_output_bw), _ = tf1.nn.bidirectional_dynamic_rnn(\n            forward_cell, backward_cell, embeddings,\n            sequence_length=self.lengths, dtype=tf.float32)\n        rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n\n        self.logits = tf1.layers.dense(rnn_output, n_tags, activation=None)\n\n    def compute_predictions(self):\n        self.softmax_output = tf.nn.softmax(self.logits)\n        self.predictions = tf.argmax(self.softmax_output, axis=-1)\n\n    def compute_loss(self, n_tags, padding_idx):\n        one_hot_true_tags = tf.one_hot(self.true_tags, n_tags)\n        loss_tensor = tf.nn.softmax_cross_entropy_with_logits(\n            one_hot_true_tags, self.logits)\n\n        mask = tf.cast(tf.not_equal(self.input_batch, padding_idx), tf.float32)\n        self.loss = tf.reduce_mean(mask * loss_tensor)\n\n    def optimize(self):\n        self.optimizer = tf1.train.AdamOptimizer(learning_rate=self.learn_rate)\n        self.gradient_var = self.optimizer.compute_gradients(self.loss)\n\n        clip_norm = tf.cast(1.0, tf.float32)\n        self.gradient_var = [(tf.clip_by_norm(grad, clip_norm), var) for\n                             (grad, var) in self.gradient_var]\n        self.train_op = self.optimizer.apply_gradients(self.gradient_var)\n\n    def train_batch(self, session, tokens, tags, lengths,\n                    learn_rate, dropout):\n        placeholders = {self.input_batch: tokens,\n                        self.true_tags: tags,\n                        self.learn_rate: learn_rate,\n                        self.dropout: dropout,\n                        self.lengths: lengths}\n\n        session.run(self.train_op, feed_dict=placeholders)\n\n    def predict_batch(self, session, tokens, lengths):\n        placeholders = {self.input_batch: tokens,\n                        self.lengths: lengths}\n\n        predictions = session.run(self.predictions, feed_dict=placeholders)\n        softmax_output = session.run(self.softmax_output,\n                                     feed_dict=placeholders)\n\n        return predictions, softmax_output\n\n\nclass TransformerNER():\n\n    def __init__(self, epochs, batch_size, labels):\n        self.model_args = NERArgs(num_train_epochs=epochs,\n                                  train_batch_size=batch_size,\n                                  eval_batch_size=batch_size,\n                                  evaluate_during_training=True,\n                                  output_dir='/output/transformerNER',\n                                  best_model_dir='/output/transformerNER/best',\n                                  overwrite_output_dir=True,\n                                  fp16=False,\n                                  labels_list=labels,\n                                  do_lower_case=True)\n\n        self.model = NERModel(\n            'bert', 'cahya/bert-base-indonesian-1.5G',\n            labels=labels, args=self.model_args\n        )\n\n    def train(self, train_data, eval_data):\n        self.model.train_model(train_data, eval_data=eval_data)\n\n    def evaluate(self, data):\n        return self.model.eval_model(data)\n\n    def predict(self, test_data):\n        return self.model.predict(test_data)\n","metadata":{"id":"W3xvW4kKzDqN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport sys\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\ntrain = pd.read_csv('/kaggle/input/scl-address-element-extraction/train.csv')\nlabel = train.pop('POI/street').values\nraw_addr = [i[0].lstrip(' ').rstrip(' ') for i in train.drop(columns=['id'], axis=1).values]\n\npoi_tokens = []\nstreet_tokens = []\n\nfor data in label:\n    splitted = data.split('/')\n    poi_tokens.append(re.findall(r'[\\w]+|[^\\s\\w]', splitted[0]))\n    street_tokens.append(re.findall(r'[\\w]+|[^\\s\\w]', splitted[1]))\n\nraw_addr = [re.findall(r'[\\w]+|[^\\s\\w]', i) for i in raw_addr]\n\ntokens = []\ntags = []\n\ntoken_dict = {}\n\ntag_set = ['O', 'B-POI', 'B-STREET', 'I-POI', 'I-STREET']\n\nfor i in range(len(raw_addr)):\n    tokens_lst = raw_addr[i].copy()\n    tags_lst = ['O' for i in range(len(raw_addr[i]))]\n\n    for j in range(len(tokens_lst)):\n        match = 1\n        for k in range(len(poi_tokens[i])):\n            if j + k >= len(tokens_lst):\n                match = 0\n                break\n\n            regex = '^' + re.escape(tokens_lst[j + k])\n            matched = bool(re.match(regex, poi_tokens[i][k]))\n            if matched is False:\n                match = 0\n                break\n\n        if match == 1 and len(poi_tokens[i]) > 0:\n            for k in range(len(poi_tokens[i])):\n                \n                if k == 0:\n                    tags_lst[j + k] = 'B-POI'\n                else:\n                    tags_lst[j + k] = 'I-POI'\n\n                if tokens_lst[j + k] in token_dict:\n                    if poi_tokens[i][k] in token_dict[tokens_lst[j + k]]:\n                        token_dict[tokens_lst[j + k]][poi_tokens[i][k]] += 1\n                    else:\n                        token_dict[tokens_lst[j + k]][poi_tokens[i][k]] = 1\n                else:\n                    token_dict[tokens_lst[j + k]] = {}\n                    token_dict[tokens_lst[j + k]][poi_tokens[i][k]] = 1\n\n                tokens_lst[j + k] = poi_tokens[i][k]\n\n            break\n\n    for j in range(len(tokens_lst)):\n        match = 1\n        for k in range(len(street_tokens[i])):\n            if j + k >= len(tokens_lst):\n                match = 0\n                break\n\n            if tags_lst[j + k] != 'O':\n                match = 0\n                break\n\n            regex = '^' + re.escape(tokens_lst[j + k])\n            matched = bool(re.match(regex, street_tokens[i][k]))\n            if matched is False:\n                match = 0\n                break\n\n        if match == 1 and len(street_tokens[i]) > 0:\n            for k in range(len(street_tokens[i])):\n                \n                if k == 0:\n                    tags_lst[j + k] = 'B-STREET'\n                else:\n                    tags_lst[j + k] = 'I-STREET'\n\n                if tokens_lst[j + k] in token_dict:\n                    if street_tokens[i][k] in token_dict[tokens_lst[j + k]]:\n                        token_dict[tokens_lst[j + k]][street_tokens[i][k]] += 1\n                    else:\n                        token_dict[tokens_lst[j + k]][street_tokens[i][k]] = 1\n                else:\n                    token_dict[tokens_lst[j + k]] = {}\n                    token_dict[tokens_lst[j + k]][street_tokens[i][k]] = 1\n\n                tokens_lst[j + k] = street_tokens[i][k]\n\n            break\n\n    for j in range(len(tokens_lst)):\n        if tokens_lst[j].isdigit():\n            tokens_lst[j] = '15'\n\n    tokens.append(tokens_lst)\n    tags.append(tags_lst)\n\n    # print(i)\n\ntrain_tokens, valid_tokens, train_tags, valid_tags = train_test_split(tokens,\n    tags, test_size=0.2)\n\ntrain_sent_id = [i for i in range(len(train_tokens))\n                 for j in range(len(train_tokens[i]))]\ntrain_tokens = [token for token_lst in train_tokens for token in token_lst]\ntrain_tags = [tag for tag_lst in train_tags for tag in tag_lst]\ntrain_df = pd.DataFrame(list(zip(train_sent_id, train_tokens, train_tags)),\n                        columns=['sentence_id', 'words', 'labels'])\n\nvalid_sent_id = [i for i in range(len(valid_tokens))\n                 for j in range(len(valid_tokens[i]))]\nvalid_tokens = [token for token_lst in valid_tokens for token in token_lst]\nvalid_tags = [tag for tag_lst in valid_tags for tag in tag_lst]\nvalid_df = pd.DataFrame(list(zip(valid_sent_id, valid_tokens, valid_tags)),\n                        columns=['sentence_id', 'words', 'labels'])\n","metadata":{"id":"gOUNtRhzzNxH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TransformerNER(4, 64, tag_set)","metadata":{"id":"BNfOMK29oe-D","outputId":"579b6021-acd8-49c6-e112-2d9128e33632","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train(train_df, valid_df)","metadata":{"id":"5XDI_lqfQl3_","outputId":"33bdb97d-7c1d-4330-fd91-5b6bf636bc0d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/scl-address-element-extraction/train.csv')\nlabel = train.pop('POI/street').values\nraw_addr = [i[0].lstrip(' ').rstrip(' ') for i in train.drop(columns=['id'], axis=1).values]\n\npoi_tokens = []\nstreet_tokens = []\n\nfor data in label:\n    splitted = data.split('/')\n    poi_tokens.append(re.findall(r'[\\w]+|[^\\s\\w]', splitted[0]))\n    street_tokens.append(re.findall(r'[\\w]+|[^\\s\\w]', splitted[1]))\n\nraw_addr = [re.findall(r'[\\w]+|[^\\s\\w]', i) for i in raw_addr]\n\ntokens = []\ntags = []\n\ntoken_dict = {}\n\ntag_set = ['O', 'B-POI', 'B-STREET', 'I-POI', 'I-STREET']\n\nfor i in range(len(raw_addr)):\n    tokens_lst = raw_addr[i].copy()\n    tags_lst = ['O' for i in range(len(raw_addr[i]))]\n\n    for j in range(len(tokens_lst)):\n        match = 1\n        for k in range(len(street_tokens[i])):\n            if j + k >= len(tokens_lst):\n                match = 0\n                break\n\n            regex = '^' + re.escape(tokens_lst[j + k])\n            matched = bool(re.match(regex, street_tokens[i][k]))\n            if matched is False:\n                match = 0\n                break\n\n        if match == 1 and len(street_tokens[i]) > 0:\n            for k in range(len(street_tokens[i])):\n                \n                if k == 0:\n                    tags_lst[j + k] = 'B-STREET'\n                else:\n                    tags_lst[j + k] = 'I-STREET'\n\n                if tokens_lst[j + k] in token_dict:\n                    if street_tokens[i][k] in token_dict[tokens_lst[j + k]]:\n                        token_dict[tokens_lst[j + k]][street_tokens[i][k]] += 1\n                    else:\n                        token_dict[tokens_lst[j + k]][street_tokens[i][k]] = 1\n                else:\n                    token_dict[tokens_lst[j + k]] = {}\n                    token_dict[tokens_lst[j + k]][street_tokens[i][k]] = 1\n\n                tokens_lst[j + k] = street_tokens[i][k]\n\n            break\n\n    for j in range(len(tokens_lst)):\n        match = 1\n        for k in range(len(poi_tokens[i])):\n            if j + k >= len(tokens_lst):\n                match = 0\n                break\n\n            if tags_lst[j + k] != 'O':\n                match = 0\n                break\n\n            regex = '^' + re.escape(tokens_lst[j + k])\n            matched = bool(re.match(regex, poi_tokens[i][k]))\n            if matched is False:\n                match = 0\n                break\n\n        if match == 1 and len(poi_tokens[i]) > 0:\n            for k in range(len(poi_tokens[i])):\n                \n                if k == 0:\n                    tags_lst[j + k] = 'B-POI'\n                else:\n                    tags_lst[j + k] = 'I-POI'\n\n                if tokens_lst[j + k] in token_dict:\n                    if poi_tokens[i][k] in token_dict[tokens_lst[j + k]]:\n                        token_dict[tokens_lst[j + k]][poi_tokens[i][k]] += 1\n                    else:\n                        token_dict[tokens_lst[j + k]][poi_tokens[i][k]] = 1\n                else:\n                    token_dict[tokens_lst[j + k]] = {}\n                    token_dict[tokens_lst[j + k]][poi_tokens[i][k]] = 1\n\n                tokens_lst[j + k] = poi_tokens[i][k]\n\n            break\n\n    for j in range(len(tokens_lst)):\n        if tokens_lst[j].isdigit():\n            tokens_lst[j] = '15'\n\n    tokens.append(tokens_lst)\n    tags.append(tags_lst)\n\n    # print(i)\n\ntrain_tokens, valid_tokens, train_tags, valid_tags = train_test_split(tokens,\n    tags, test_size=0.2)\n\ntrain_sent_id = [i for i in range(len(train_tokens))\n                 for j in range(len(train_tokens[i]))]\ntrain_tokens = [token for token_lst in train_tokens for token in token_lst]\ntrain_tags = [tag for tag_lst in train_tags for tag in tag_lst]\ntrain_df = pd.DataFrame(list(zip(train_sent_id, train_tokens, train_tags)),\n                        columns=['sentence_id', 'words', 'labels'])\n\nvalid_sent_id = [i for i in range(len(valid_tokens))\n                 for j in range(len(valid_tokens[i]))]\nvalid_tokens = [token for token_lst in valid_tokens for token in token_lst]\nvalid_tags = [tag for tag_lst in valid_tags for tag in tag_lst]\nvalid_df = pd.DataFrame(list(zip(valid_sent_id, valid_tokens, valid_tags)),\n                        columns=['sentence_id', 'words', 'labels'])","metadata":{"id":"T7UT2U6Ha01R","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = TransformerNER(3, 64, tag_set)","metadata":{"id":"nEIZC4KrbXC6","outputId":"86b8545d-f067-4dce-dfc9-8a9f4e995b71","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ntorch.cuda.empty_cache()","metadata":{"id":"bIdVtXSHBJSt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.train(train_df, valid_df)","metadata":{"id":"z_P2BNcvbhHP","outputId":"74bd28f2-9733-4ed1-e978-2ec382dd346f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for tokens in token_dict:\n    max_key = max(token_dict[tokens], key=token_dict[tokens].get)\n    token_dict[tokens] = max_key","metadata":{"id":"P_4qzPkw7sYx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/scl-address-element-extraction/test.csv').drop(columns=['id'], axis=1).values\ntest = [i[0].lstrip(' ').rstrip(' ') for i in test]\n\ntest_split = [re.findall(r'[\\w]+|[^\\s\\w]', i) for i in test]\ntest_split_corrected = [re.findall(r'[\\w]+|[^\\s\\w]', i) for i in test]\n\nfor i in range(len(test_split)):\n    for j in range(len(test_split[i])):\n        if test_split[i][j] in token_dict:\n            test_split_corrected[i][j] = token_dict[test_split[i][j]]\n\ntest_split_intermediary = []\ntest_text = []\n\nfor i in range(len(test_split)):\n  pos = 0\n  test_split_list = []\n  text = ''\n  original_text = ''\n  for j in range(len(test_split[i])):\n      tmp = ''\n      while test[i][pos] == ' ':\n          pos += 1\n          tmp += ' '\n      test_split_list.append(tmp)\n      pos += len(test_split[i][j])\n      if test_split_corrected[i][j].isdigit():\n          text += tmp + ' 15'\n      else:\n          text += tmp + ' ' + test_split_corrected[i][j]\n      original_text += tmp + ' ' + test_split_corrected[i][j]\n    \n  test[i] = original_text\n  test_text.append(text)\n  test_split_intermediary.append(test_split_list)\n","metadata":{"id":"B0-ikn7c8A5F","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_predictions = model.predict(test_text)\npredictions = raw_predictions[0]","metadata":{"id":"5FuQnmeOKr3p","outputId":"51d67be0-da0b-4a95-c65f-c389bbead561","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"id":"6mUMDz_Q94V8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_intermediary = []\nprediction_tokens = []\n\nfor i in range(len(predictions)):\n    pos = 0\n    text_pos = 0\n    intermed = []\n    tokens = []\n    for j in range(len(predictions[i])):\n        tmp = ''\n        while test_text[i][pos] == ' ':\n            pos += 1\n            text_pos += 1\n            tmp += ' '\n        pred = list(predictions[i][j].keys())[0]\n        pos += len(pred)\n        # print(pred, len(tmp), pos, test_text[i])\n        if pred == '15':\n            pred = ''\n            # print(test[i])\n            for k in range(text_pos, len(test[i])):\n                if test[i][k].isdigit() is True:\n                    pred += test[i][k]\n                    text_pos = k+1\n                else:\n                    # print(test[i])\n                    break\n            # if next(iter(predictions[i][j].values())) != 'O':\n            #     print('OK')\n        else:\n            text_pos += len(pred)\n        tmp = tmp[:-1]\n        intermed.append(tmp)\n        tokens.append(pred)\n    prediction_intermediary.append(intermed)\n    prediction_tokens.append(tokens)","metadata":{"id":"ji8uXf7tUAbB","outputId":"6a38e000-7201-4e97-ad01-9b601f0d6385","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_poi = []\n\nfor i in range(len(predictions)):\n    res = ''\n    start_idx = -1\n    end_idx = -1\n    tmp = ''\n    for j in range(len(predictions[i])):\n        tag = next(iter(predictions[i][j].values()))\n        if tag == 'B-POI':\n            start_idx = j\n            end_idx = j\n            for k in range(j + 1, len(predictions[i])):\n                tag = next(iter(predictions[i][k].values()))\n                if tag == 'I-POI':\n                    end_idx = k\n                else:\n                    break\n\n            break\n\n    if start_idx != -1:\n        for j in range(start_idx, end_idx + 1):\n            tmp += prediction_intermediary[i][j] + prediction_tokens[i][j]\n\n        res += tmp.lstrip(' ')\n\n    # res += '/'\n    # start_idx = -1\n    # end_idx = -1\n    # tmp = ''\n\n    # for j in range(len(predictions[i])):\n    #     tag = next(iter(predictions[i][j].values()))\n    #     if tag == 'B-STREET':\n    #         start_idx = j\n    #         end_idx = j\n    #         for k in range(j + 1, len(predictions[i])):\n    #             tag = next(iter(predictions[i][k].values()))\n    #             if tag == 'I-STREET':\n    #                 end_idx = k\n    #             else:\n    #                 break\n\n    #         break\n\n    # if start_idx != -1:\n    #     tmp = ''\n    #     for j in range(start_idx, end_idx + 1):\n    #         tmp += prediction_intermediary[i][j] + prediction_tokens[i][j]\n\n    #     res += tmp.lstrip(' ')\n\n    result_poi.append(res)\n\nresult_poi","metadata":{"id":"jQhb0AzzOADH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"JPkE6-pCBXoa","outputId":"b251947b-0062-447e-bb46-d5273bd84573","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_predictions2 = model.predict(test_text)\npredictions2 = raw_predictions2[0]","metadata":{"id":"LqyAaUL3bsSD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_street = []\n\nfor i in range(len(predictions2)):\n    # res = ''\n    # start_idx = -1\n    # end_idx = -1\n    # tmp = ''\n    # for j in range(len(predictions2[i])):\n    #     tag = next(iter(predictions2[i][j].values()))\n    #     if tag == 'B-POI':\n    #         start_idx = j\n    #         end_idx = j\n    #         for k in range(j + 1, len(predictions2[i])):\n    #             tag = next(iter(predictions2[i][k].values()))\n    #             if tag == 'I-POI':\n    #                 end_idx = k\n    #             else:\n    #                 break\n\n    #         break\n\n    # if start_idx != -1:\n    #     for j in range(start_idx, end_idx + 1):\n    #         tmp += prediction_intermediary[i][j] + prediction_tokens[i][j]\n\n    #     res += tmp.lstrip(' ')\n\n    res = '/'\n    start_idx = -1\n    end_idx = -1\n    tmp = ''\n\n    for j in range(len(predictions2[i])):\n        tag = next(iter(predictions2[i][j].values()))\n        if tag == 'B-STREET':\n            start_idx = j\n            end_idx = j\n            for k in range(j + 1, len(predictions2[i])):\n                tag = next(iter(predictions2[i][k].values()))\n                if tag == 'I-STREET':\n                    end_idx = k\n                else:\n                    break\n\n            break\n\n    if start_idx != -1:\n        tmp = ''\n        for j in range(start_idx, end_idx + 1):\n            tmp += prediction_intermediary[i][j] + prediction_tokens[i][j]\n\n        res += tmp.lstrip(' ')\n\n    result_street.append(res)\n\nresult_street","metadata":{"id":"OoCPUleab1zZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = [''.join([a, b]) for a,b in zip(result_poi, result_street)]","metadata":{"id":"VuLMW82bcFsu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(result, columns=['POI/street'])\ndf","metadata":{"id":"xYLfh9K07Fom","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('submission.csv', index=True, index_label='id', columns=['POI/street'])","metadata":{"id":"UAWJUuf97_tB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'../working')\nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}